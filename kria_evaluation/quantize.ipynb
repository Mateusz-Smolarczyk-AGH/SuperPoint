{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZL5HI3Rj3blg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import local_utils\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from local_utils import ResidualBlock\n",
    "from torch import nn\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from types import SimpleNamespace\n",
    "import torch.quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W tej części ćwiczenia, wczytamy nauczony model zmiennoprzecinkowy MiniResNet, skwantyzujemy go do postaci stałoprzecinkowej i na koniec skompilujemy go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dane ewaluacyjne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczynamy od ponownego stworzenia generatora danych na bazie danych MNIST:\n",
    "\n",
    "Wystarczy nam sama część testowa. Ustawiamy `batch_size` na 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba obrazów w zbiorze testowym: 6\n"
     ]
    }
   ],
   "source": [
    "class SuperPointDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_paths = [str(f) for f in Path(image_folder).iterdir() if f.suffix == \".ppm\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (300, 200))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.image_paths[idx]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "test_dataset = SuperPointDataset(\"./data\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"Liczba obrazów w zbiorze testowym: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo przygotujemy plik w formacie `.npz` na podstawie danych testowych. Wykorzystamy go do ewaluacji modelu na docelowej platformie Kria.\n",
    "\n",
    "Uzupełnij wektory `quantization_data` oraz `quantization_labels` danymi z `test_loadera`. Wykorzystaj do tego pętle `for` oraz `.append` (Przykład wykorzystania DataLoader'a z pętlą `for` przedstawiono w 1 części podczas wczytywania danych).\n",
    "\n",
    "Następnie każdy wektor z osobna połącz funkcją `torch.cat` z parametrem `dim=0` i przekonwertuj je do formatu `ndarray` za pomocą `.numpy()`. \n",
    "\n",
    "Zapisz je funkcją np.savez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantization_data = []\n",
    "#quantization_labels = []\n",
    "\n",
    "#TODO\n",
    "#Fill quantization vectors\n",
    "\n",
    "#train_X = ... #TODO\n",
    "#train_Y = ... #TODO\n",
    "\n",
    "#np.savez('eval_MNIST.npz', data=..., targets=...) #TODO\n",
    "\n",
    "#print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inicjalizacja modelu zmiennoprzecinkowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy taką samą klasę sieci Superpoint jak w pierwszej części ćwiczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_descriptors(keypoints, descriptors, s: int = 8):\n",
    "    \"\"\"Interpolate descriptors at keypoint locations\"\"\"\n",
    "    b, c, h, w = descriptors.shape\n",
    "    keypoints = (keypoints + 0.5) / (keypoints.new_tensor([w, h]) * s)\n",
    "    keypoints = keypoints * 2 - 1  # normalize to (-1, 1)\n",
    "    descriptors = torch.nn.functional.grid_sample(\n",
    "        descriptors, keypoints.view(b, 1, -1, 2), mode=\"bilinear\", align_corners=False\n",
    "    )\n",
    "    descriptors = torch.nn.functional.normalize(\n",
    "        descriptors.reshape(b, c, -1), p=2, dim=1\n",
    "    )\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def batched_nms(scores, nms_radius: int):\n",
    "    assert nms_radius >= 0\n",
    "\n",
    "    def max_pool(x):\n",
    "        return torch.nn.functional.max_pool2d(\n",
    "            x, kernel_size=nms_radius * 2 + 1, stride=1, padding=nms_radius\n",
    "        )\n",
    "\n",
    "    zeros = torch.zeros_like(scores)\n",
    "    max_mask = scores == max_pool(scores)\n",
    "    for _ in range(2):\n",
    "        supp_mask = max_pool(max_mask.float()) > 0\n",
    "        supp_scores = torch.where(supp_mask, zeros, scores)\n",
    "        new_max_mask = supp_scores == max_pool(supp_scores)\n",
    "        max_mask = max_mask | (new_max_mask & (~supp_mask))\n",
    "    return torch.where(max_mask, scores, zeros)\n",
    "\n",
    "\n",
    "def select_top_k_keypoints(keypoints, scores, k):\n",
    "    if k >= len(keypoints):\n",
    "        return keypoints, scores\n",
    "    scores, indices = torch.topk(scores, k, dim=0, sorted=True)\n",
    "    return keypoints[indices], scores\n",
    "\n",
    "\n",
    "class VGGBlock(nn.Sequential):\n",
    "    def __init__(self, c_in, c_out, kernel_size, relu=True):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        conv = nn.Conv2d(\n",
    "            c_in, c_out, kernel_size=kernel_size, stride=1, padding=padding\n",
    "        )\n",
    "        activation = nn.ReLU(inplace=True) if relu else nn.Identity()\n",
    "        bn = nn.BatchNorm2d(c_out, eps=0.001)\n",
    "        super().__init__(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"conv\", conv),\n",
    "                    (\"activation\", activation),\n",
    "                    (\"bn\", bn),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class SuperPoint(nn.Module):\n",
    "    default_conf = {\n",
    "        \"nms_radius\": 4,\n",
    "        \"max_num_keypoints\": 500,\n",
    "        \"detection_threshold\": 0.005,\n",
    "        \"remove_borders\": 4,\n",
    "        \"descriptor_dim\": 256,\n",
    "        \"channels\": [64, 64, 128, 128, 256],\n",
    "    }\n",
    "\n",
    "    def __init__(self, **conf):\n",
    "        super().__init__()\n",
    "        conf = {**self.default_conf, **conf}\n",
    "        self.conf = SimpleNamespace(**conf)\n",
    "        self.stride = 2 ** (len(self.conf.channels) - 2)\n",
    "        channels = [1, *self.conf.channels[:-1]]\n",
    "\n",
    "        backbone = []\n",
    "        for i, c in enumerate(channels[1:], 1):\n",
    "            layers = [VGGBlock(channels[i - 1], c, 3), VGGBlock(c, c, 3)]\n",
    "            if i < len(channels) - 1:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            backbone.append(nn.Sequential(*layers))\n",
    "        self.backbone = nn.Sequential(*backbone)\n",
    "\n",
    "        c = self.conf.channels[-1]\n",
    "        self.detector = nn.Sequential(\n",
    "            VGGBlock(channels[-1], c, 3),\n",
    "            VGGBlock(c, self.stride**2 + 1, 1, relu=False),\n",
    "        )\n",
    "        self.descriptor = nn.Sequential(\n",
    "            VGGBlock(channels[-1], c, 3),\n",
    "            VGGBlock(c, self.conf.descriptor_dim, 1, relu=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        image = data\n",
    "        if image.shape[1] == 3:  # RGB to gray\n",
    "            scale = image.new_tensor([0.299, 0.587, 0.114]).view(1, 3, 1, 1)\n",
    "            image = (image * scale).sum(1, keepdim=True)\n",
    "\n",
    "        features = self.backbone(image)\n",
    "        descriptors_dense = torch.nn.functional.normalize(\n",
    "            self.descriptor(features), p=2, dim=1\n",
    "        )\n",
    "\n",
    "        # Decode the detection scores\n",
    "        scores = self.detector(features)\n",
    "\n",
    "        scores = torch.nn.functional.softmax(scores, 1)[:, :-1]\n",
    "\n",
    "        b, _, h, w = scores.shape\n",
    "        scores = scores.permute(0, 2, 3, 1).reshape(b, h, w, self.stride, self.stride)\n",
    "        scores = scores.permute(0, 1, 3, 2, 4).reshape(\n",
    "            b, h * self.stride, w * self.stride\n",
    "        )\n",
    "\n",
    "        scores = batched_nms(scores, self.conf.nms_radius)\n",
    "       # lol = scores.numpy()\n",
    "       \n",
    "        # Discard keypoints near the image borders\n",
    "        if self.conf.remove_borders:\n",
    "            pad = self.conf.remove_borders\n",
    "            scores[:, :pad] = -1\n",
    "            scores[:, :, :pad] = -1\n",
    "            scores[:, -pad:] = -1\n",
    "            scores[:, :, -pad:] = -1\n",
    "\n",
    "        # Extract keypoints\n",
    "        if b > 1:\n",
    "            idxs = torch.where(scores > self.conf.detection_threshold)\n",
    "            mask = idxs[0] == torch.arange(b, device=scores.device)[:, None]\n",
    "        else:  # Faster shortcut\n",
    "            scores = scores.squeeze(0)\n",
    "            idxs = torch.where(scores > self.conf.detection_threshold)\n",
    "\n",
    "        # Convert (i, j) to (x, y)\n",
    "        keypoints_all = torch.stack(idxs[-2:], dim=-1).flip(1).float()\n",
    "        scores_all = scores[idxs]\n",
    "\n",
    "        keypoints = []\n",
    "        scores = []\n",
    "        descriptors = []\n",
    "        for i in range(b):\n",
    "            if b > 1:\n",
    "                k = keypoints_all[mask[i]]\n",
    "                s = scores_all[mask[i]]\n",
    "            else:\n",
    "                k = keypoints_all\n",
    "                s = scores_all\n",
    "            if self.conf.max_num_keypoints is not None:\n",
    "                k, s = select_top_k_keypoints(k, s, self.conf.max_num_keypoints)\n",
    "            d = sample_descriptors(k[None], descriptors_dense[i, None], self.stride)\n",
    "            keypoints.append(k)\n",
    "            scores.append(s)\n",
    "            descriptors.append(d.squeeze(0).transpose(0, 1))\n",
    "\n",
    "        return {\n",
    "            \"keypoints\": keypoints,\n",
    "            \"keypoint_scores\": scores,\n",
    "            \"descriptors\": descriptors,\n",
    "        }\n",
    "\n",
    "    \n",
    "class SuperPoint_short(nn.Module):\n",
    "    default_conf = {\n",
    "        \"nms_radius\": 4,\n",
    "        \"max_num_keypoints\": 500,\n",
    "        \"detection_threshold\": 0.005,\n",
    "        \"remove_borders\": 4,\n",
    "        \"descriptor_dim\": 256,\n",
    "        \"channels\": [64, 64, 128, 128, 256],\n",
    "    }\n",
    "\n",
    "    def __init__(self, **conf):\n",
    "        super().__init__()\n",
    "        conf = {**self.default_conf, **conf}\n",
    "        self.conf = conf\n",
    "        self.stride = 2 ** (len(self.conf[\"channels\"]) - 2)\n",
    "        channels = [1, *self.conf[\"channels\"][:-1]]\n",
    "\n",
    "        # Definicja QuantStub\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        backbone = []\n",
    "        for i, c in enumerate(channels[1:], 1):\n",
    "            layers = [VGGBlock(channels[i - 1], c, 3), VGGBlock(c, c, 3)]\n",
    "            if i < len(channels) - 1:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            backbone.append(nn.Sequential(*layers))\n",
    "        self.backbone = nn.Sequential(*backbone)\n",
    "\n",
    "        c = self.conf[\"channels\"][-1]\n",
    "        self.detector = nn.Sequential(\n",
    "            VGGBlock(channels[-1], c, 3),\n",
    "            VGGBlock(c, self.stride**2 + 1, 1, relu=False),\n",
    "        )\n",
    "        self.descriptor = nn.Sequential(\n",
    "            VGGBlock(channels[-1], c, 3),\n",
    "            VGGBlock(c, self.conf[\"descriptor_dim\"], 1, relu=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Kwantyzacja wejścia\n",
    "        image = self.quant(data)\n",
    "\n",
    "        features = self.backbone(image)\n",
    "        descriptors_dense = torch.nn.functional.normalize(\n",
    "            self.descriptor(features), p=2, dim=1\n",
    "        )\n",
    "\n",
    "        # Decode the detection scores\n",
    "        scores = self.detector(features)\n",
    "\n",
    "        scores = torch.nn.functional.softmax(scores, 1)[:, :-1]\n",
    "\n",
    "        b, _, h, w = scores.shape\n",
    "        scores = scores.permute(0, 2, 3, 1).reshape(b, h, w, self.stride, self.stride)\n",
    "        scores = scores.permute(0, 1, 3, 2, 4).reshape(\n",
    "            b, h * self.stride, w * self.stride\n",
    "        )\n",
    "\n",
    "        # Dekwantyzacja przed wyjściem\n",
    "        return self.dequant(scores), self.dequant(descriptors_dense)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy model i wgrywamy wagi z pliku MNIST.pth. Zapisujemy go do urządzenia (w dockerze dostępny jest tylko CPU!) i ustawiamy go na ewaluację `.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H1qAmZ7D3jMx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "detection_thresh = 0.005\n",
    "nms_radius = 5\n",
    "\n",
    "float_model = SuperPoint_short(detection_threshold=detection_thresh, nms_radius=nms_radius).eval()\n",
    "float_model.load_state_dict(torch.load(\"model_weights_legacy.pth\"))\n",
    "\n",
    "input_shape = (1, 1, 200, 300)\n",
    "model = float_model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ewaluacja modelu zmiennoprzecinkowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przed przystąpieniem do kwantyzacji, wykonamy szybką ewaluację modelu zmiennoprzecinkowego. Sprawdzimy, czy dane są dobrze przygotowane i czy model został odpowiednio zapisany i wczytany. Wczytujemy metrykę Accuracy z `local_utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L4tpsFQX4jip"
   },
   "outputs": [],
   "source": [
    "def evaluate_orig(model,\n",
    "             dataloader):\n",
    "    tm = local_utils.TimeMeasurement(\"Evaluation\", len(dataloader))\n",
    "    with torch.no_grad(), tm:\n",
    "        score = 0.0\n",
    "        cntr = 0\n",
    "        for i, XY in enumerate(dataloader):\n",
    "            X = XY[0]\n",
    "            Y = XY[1:]\n",
    "            y_pred = model(X)\n",
    "            score = score*cntr + X.shape[0]*evaluator(y_pred, *Y)\n",
    "            cntr += X.shape[0]\n",
    "            score /= cntr\n",
    "            print(\"\\rEvaluation {}/{}. Score = {}\".format(i,len(dataloader), score),end='')\n",
    "        \n",
    "        print(\"\\rEvaluation {}/{}. Score = {}\".format(len(dataloader),len(dataloader), score),end='\\n')\n",
    "    print(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Porównuje działanie modelu przed i po kwantyzacji.\n",
    "\n",
    "    :param float_model: model przed kwantyzacją\n",
    "    :param quantized_model: model po kwantyzacji\n",
    "    :param dataloader: zbiór testowy\n",
    "    :param device: urządzenie (CPU/GPU)\n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Evaluation\", len(dataloader))\n",
    "    \n",
    "    model.to(device).eval()\n",
    "\n",
    "    mae_total, mse_total, count = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad(), tm:\n",
    "        for i, (X, *Y) in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            # Forward pass dla modeli\n",
    "            scores, descriptors_dense = model(X)\n",
    "            torch.save({'scores': scores, 'descriptors_dense': descriptors_dense}, f'outputs_batch_{i}.pt')\n",
    "            data = torch.load(f'data/outputs_batch_{i}.pt')\n",
    "\n",
    "            # Obliczenie różnicy\n",
    "            mae = torch.mean(torch.abs(scores - data['scores'])).item()\n",
    "            mse = F.mse_loss(scores, data['scores']).item()\n",
    "            \n",
    "            mae += torch.mean(torch.abs(descriptors_dense - data['descriptors_dense'])).item()\n",
    "            mse += F.mse_loss(descriptors_dense, data['descriptors_dense']).item()\n",
    "            \n",
    "            # Aktualizacja wyników\n",
    "            batch_size = X.shape[0]\n",
    "            mae_total += mae * batch_size\n",
    "            mse_total += mse * batch_size\n",
    "            count += batch_size\n",
    "\n",
    "            print(f\"\\rEvaluation {i+1}/{len(dataloader)} | MAE: {mae:.6f} | MSE: {mse:.6f}\", end='')\n",
    "\n",
    "    # Średnie wyniki\n",
    "    mae_avg = mae_total / count\n",
    "    mse_avg = mse_total / count\n",
    "\n",
    "    print(f\"\\nFinal Evaluation: MAE = {mae_avg:.6f}, MSE = {mse_avg:.6f}\")\n",
    "    print(tm)\n",
    "\n",
    "    return mae_avg, mse_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation 6/6 | MAE: 0.000000 | MSE: 0.000000\n",
      "Final Evaluation: MAE = 0.000000, MSE = 0.000000\n",
      "Execution time: 0.0:0.0:0:840, processed 6 frames, throughput: 7.13833969895999 fps.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can evaluate your floating point model first \n",
    "evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jeżeli wszystko działa poprawnie, a uzyskana dokładność jest na podobnym poziome jak podczas uczenia, możemy przejść do kwantyzacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Kwantyzacja modelu zmiennoprzecinkowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitis AI Quantizer dla Post Training Quantization składa się z dwóch części.\n",
    "Pierwszą częścią jest kalibracja (mode='calib') - Vitis AI Quantizer analizuje model i dostosowuje parametry kwantyzacji.\n",
    " \n",
    "Drugą częścią jest ewaluacja/testowanie (mode='test') - sprawdzana jest dokładność modelu (nie powinna być duża zmiana) i model jest eksportowany do formatu .xmodel.\n",
    "\n",
    "### Do obu części wykorzystamy funkcję quantize.\n",
    "\n",
    "Funkcja wykorzystuje kwantyzator dla PyTorch z gita Vitis AI: https://github.com/Xilinx/Vitis-AI/tree/1.4/tools/Vitis-AI-Quantizer/vai_q_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QFuGuk2NB4fz",
    "outputId": "3aea7dc1-a5e2-4f08-e7e2-f2af247666e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device,\n",
    "             dataloader):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader - for 'calib' must be batch_size == 1\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Quantization\", len(dataloader))\n",
    "    from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "\n",
    "    with tm:\n",
    "        # model to device\n",
    "        model = float_model.to(device)\n",
    "\n",
    "        # Force to merge BN with CONV for better quantization accuracy\n",
    "        optimize = 1\n",
    "\n",
    "        rand_in = torch.randn(input_shape)\n",
    "        print(\"get qunatizer start\")\n",
    "        try:\n",
    "            quantizer = torch_quantizer(\n",
    "                quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "        except Exception as e:\n",
    "            print(\"exception:\")\n",
    "            print(e)\n",
    "            return\n",
    "        print(\"get qunatizer end\")\n",
    "\n",
    "        print(\"get quantized model start\")\n",
    "        quantized_model = quantizer.quant_model\n",
    "        print(\"get quantized model end\")\n",
    "\n",
    "        # evaluate\n",
    "        print(\"testing start\")\n",
    "        mae, mse = evaluate(quantized_model, dataloader, device)\n",
    "        print(f\"Testing finished: MAE = {mae:.6f}, MSE = {mse:.6f}\")\n",
    "        print(\"testing end\")\n",
    "\n",
    "        # export config\n",
    "        if quant_mode == 'calib':\n",
    "            print(\"export config\")\n",
    "            quantizer.export_quant_config()\n",
    "            print(\"export config end\")\n",
    "        # export model\n",
    "        if quant_mode == 'test':\n",
    "            print(\"export xmodel\")\n",
    "            quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "            print(\"export xmodel end\")\n",
    "    print(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczynamy od kalibracji. Jako wejście funkcji podajemy:\n",
    "- float_model - model zmiennoprzecinkowy, który uzyskaliśmy podczas uczenia,\n",
    "- input_shape - wymiar danych wejściowych w formacie [batch, CH, W, H],\n",
    "- quant_dir - folder do którego zostanie zapisany wynik kwantyzacji,\n",
    "- quant_mode - do wyboru 'calib' albo 'test',\n",
    "- device - urządzenie na którym zostaną wykonane obliczenia (CPU),\n",
    "- dataloader - dane na którym będą wykonane obliczenia,\n",
    "- evaluator - metryka, względem której będzie sprawdzana dokładność\n",
    "\n",
    "### Uwaga! Kwantyzacja w procesie kalibracji jest wolna. W przypadku dużych modeli i dużych wymiarów danych, nie można przesadzić z ilością danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po udanej kalibracji, czas na testowanie i zapisanie modelu. Uruchamiamy funkcję ze zmienionym parametrem mode na 'test'.\n",
    "\n",
    "Proces ten jest szybszy od kalibracji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hW7mQGoL3blm",
    "outputId": "ffe3ef85-8078-4f53-87cc-62a3b036c36e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing SuperPoint_short...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quant_dir/SuperPoint_short.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing start\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-41c8950e183b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m          \u001b[0mquant_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'calib'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m          \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m          dataloader=test_loader)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-37a99d7f4bcc>\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(float_model, input_shape, quant_dir, quant_mode, device, dataloader)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Testing finished: MAE = {mae:.6f}, MSE = {mse:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c70574ffe30f>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Forward pass dla modeli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescriptors_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'descriptors_dense'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdescriptors_dense\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'outputs_batch_{i}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/outputs_batch_{i}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/quant_dir/SuperPoint_short.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_56\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_56\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_57\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_57\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_56\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_58\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_58\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_53\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_46\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_55\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_57\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_58\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_38\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.6/site-packages/pytorch_nndct/nn/modules/module_template.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m               inputs=[input])\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m           \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_quant_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# Quantize model - test - is faster\n",
    "\n",
    "#TODO\n",
    "quantize(float_model=model, \n",
    "         input_shape=input_shape,\n",
    "         quant_dir='quant_dir', # directory for quantizer results\n",
    "         quant_mode='calib',\n",
    "         device=device,\n",
    "         dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po testowaniu, należy skompilować model. Podajemy odpowiednio parametry:\n",
    "\n",
    "- --xmodel - ścieżka do zapisanego modelu (zapisany jest w folderze podanym podczas kwantyzacji jako parametr 'quant_dir'\n",
    "- --arch - podajemy plik arch.json, który znajdował się w pliku. Jest to numer (fingerprint), który określa typ DPU sprzętu docelowego\n",
    "- --net_name - nazwa naszego modelu po kompilacji - dowolna\n",
    "- --output_dir - folder do którego zapisany zostanie model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MApToNQ3bln",
    "outputId": "aa19a1c1-66a9-465f-cb22-43214b612ce5"
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "!vai_c_xir --xmodel 'quant_dir/MiniResNet_int.xmodel' --arch arch.json --net_name MiniResNet_qu --output_dir build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISD7IjSg3blp"
   },
   "source": [
    "Teraz przejdziemy do testowania modelu na sprzęcie docelowym.\n",
    "\n",
    "#### Wersja 1: Podłączenie do sieci\n",
    "Zanim podłączysz zasilanie do Kria, podepnij kabel USB do komputera, a kabel Ethernetowy do sieci, w której znajduje się komputer.\n",
    "\n",
    "Podłącz Kria do zasilania i poczekaj minutę, aż system się uruchomi.\n",
    "\n",
    "Uruchom `cutecom` z `sudo`. Włącz port odpowiadający do Kria. Jeżeli pojawi się napis `kria login:` zaloguj się:\n",
    "\n",
    "`login: ubuntu`\n",
    "\n",
    "`hasło: Xilinx123`\n",
    "\n",
    "Po zalogowaniu się, powinny pojawić się informacje systemowe. Nas interesuje adres `IPv4` dla `eth0`. Skopiuj go i dodaj do niego `:9090` - przykładowa wartość `192.168.1.26:9090`. Wklej to w przeglądarce. Powinien pojawić się kolejny Jupyter. Zaloguj się do niego hasłem:\n",
    "\n",
    "`xilinx`\n",
    "\n",
    "#### Wersja 2: Podłączenie do komputera\n",
    "Zanim podłączysz zasilanie do Kria, podepnij kabel USB do komputera oraz kabel Ethernetowy pomiędzy Kria a PC. Na PC włącz ustawienia sieci `Wired Setting` -> `IPv4` -> `Shared to other computers`. Włącz zasilanie płytki.\n",
    "Uruchom `cutecom` z `sudo`. Włącz port odpowiadający do Kria. Jeżeli pojawi się napis `kria login:` zaloguj się:\n",
    "\n",
    "`login: ubuntu`\n",
    "\n",
    "`hasło: Xilinx123`\n",
    "\n",
    "Po zalogowaniu się, powinny pojawić się informacje systemowe. Nas interesuje adres `inet` dla `eth0`. Skopiuj go i dodaj do niego `:9090` - przykładowa wartość `10.42.0.47:9090`. Wklej to w przeglądarce. Powinien pojawić się kolejny Jupyter. Zaloguj się do niego hasłem:\n",
    "\n",
    "`xilinx`\n",
    "\n",
    "#### Przesyłanie plików\n",
    "Stwórz nowy folder i nazwij go `PSRA_Lab`. Przenieś do niego odpowiednio pliki:\n",
    "- dpu.bit, \n",
    "- dpu.hwh, \n",
    "- dpu.xclbin, \n",
    "- eval_MNIST.npz lub tak jak nazwałeś swój plik z danymi do ewaluacji\n",
    "- MiniResNet_compiled.xmodel lub tak jak nazwałeś swój skompilowany plik\n",
    "\n",
    "Można to wykonać komendą `scp`, ale łatwiej jest przeciągnąć pliki z folderu do Jupyter Notebook'a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UWAGA! \n",
    "\n",
    "Jeżeli wystąpi problem z adresami IPv4 Kria (po podłaczeniu kilku płytek do jednej sieci, każda z nich będzie miała taki sam adres), należy sprawdzić, czy działa komenda w konsoli `cutecom`:\n",
    "\n",
    "`ifconfig`\n",
    "\n",
    "Jeżeli nie, to należy zainstalować poprzez:\n",
    "\n",
    " `sudo apt install net-tools`.  \n",
    " \n",
    "Po tym należy odpiąć kabel Ethernet z Kria, poczekać kilka sekund i wpisać w konsole `cutecom`:\n",
    "\n",
    "`hostname -I`\n",
    "\n",
    "Jeżeli konsola nie zwróci żadnego błędu oraz żadnego aresu IP to wpisz w konsole `cutecom`:\n",
    "\n",
    "`sudo ifconfig eth0 192.168.1.x netmask 255.255.255.0`\n",
    "\n",
    "Tutaj podany adres powinien być taki sam jak przykładowa wartość wyżej. Ustawiamy wartość `x` na inną niż była np. 123. Chcemy uniknąć konfliktu pomiędzy płytkami ale również komputerami. Po tym znowu ponawiamy:\n",
    "\n",
    "`hostname -I`.\n",
    "\n",
    "Powinien pojawić sie ustawiony przez nas adres. Podpinamy kabel Ethernet i uruchamiamy w przeglądarce Jupyter z ustawionym adresem IP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Quantize.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
