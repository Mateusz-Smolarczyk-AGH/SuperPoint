{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZL5HI3Rj3blg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import local_utils\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from local_utils import ResidualBlock\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W tej części ćwiczenia, wczytamy nauczony model zmiennoprzecinkowy MiniResNet, skwantyzujemy go do postaci stałoprzecinkowej i na koniec skompilujemy go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dane ewaluacyjne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczynamy od ponownego stworzenia generatora danych na bazie danych MNIST:\n",
    "\n",
    "Wystarczy nam sama część testowa. Ustawiamy `batch_size` na 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ... #TODO\n",
    "test_loader = ... #TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo przygotujemy plik w formacie `.npz` na podstawie danych testowych. Wykorzystamy go do ewaluacji modelu na docelowej platformie Kria.\n",
    "\n",
    "Uzupełnij wektory `quantization_data` oraz `quantization_labels` danymi z `test_loadera`. Wykorzystaj do tego pętle `for` oraz `.append` (Przykład wykorzystania DataLoader'a z pętlą `for` przedstawiono w 1 części podczas wczytywania danych).\n",
    "\n",
    "Następnie każdy wektor z osobna połącz funkcją `torch.cat` z parametrem `dim=0` i przekonwertuj je do formatu `ndarray` za pomocą `.numpy()`. \n",
    "\n",
    "Zapisz je funkcją np.savez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "quantization_data = []\n",
    "quantization_labels = []\n",
    "\n",
    "#TODO\n",
    "#Fill quantization vectors\n",
    "\n",
    "train_X = ... #TODO\n",
    "train_Y = ... #TODO\n",
    "\n",
    "np.savez('eval_MNIST.npz', data=..., targets=...) #TODO\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inicjalizacja modelu zmiennoprzecinkowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy taką samą klasę sieci MiniResNet jak w pierwszej części ćwiczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniResNet(nn.Module):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy model i wgrywamy wagi z pliku MNIST.pth. Zapisujemy go do urządzenia (w dockerze dostępny jest tylko CPU!) i ustawiamy go na ewaluację `.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H1qAmZ7D3jMx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = ... #TODO\n",
    "\n",
    "net = ... #TODO\n",
    "pretrainedModel = ... #TODO\n",
    "net.load_state_dict(pretrainedModel['model'])\n",
    "net.to(device)\n",
    "#TODO change to eval\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ewaluacja modelu zmiennoprzecinkowego"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przed przystąpieniem do kwantyzacji, wykonamy szybką ewaluację modelu zmiennoprzecinkowego. Sprawdzimy, czy dane są dobrze przygotowane i czy model został odpowiednio zapisany i wczytany. Wczytujemy metrykę Accuracy z `local_utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L4tpsFQX4jip"
   },
   "outputs": [],
   "source": [
    "def evaluate(model,\n",
    "             dataloader,\n",
    "             evaluator\n",
    "             ):\n",
    "    tm = local_utils.TimeMeasurement(\"Evaluation\", len(dataloader))\n",
    "    with torch.no_grad(), tm:\n",
    "        score = 0.0\n",
    "        cntr = 0\n",
    "        for i, XY in enumerate(dataloader):\n",
    "            X = XY[0]\n",
    "            Y = XY[1:]\n",
    "            y_pred = model(X)\n",
    "            score = score*cntr + X.shape[0]*evaluator(y_pred, *Y)\n",
    "            cntr += X.shape[0]\n",
    "            score /= cntr\n",
    "            print(\"\\rEvaluation {}/{}. Score = {}\".format(i,len(dataloader), score),end='')\n",
    "        \n",
    "        print(\"\\rEvaluation {}/{}. Score = {}\".format(len(dataloader),len(dataloader), score),end='\\n')\n",
    "    print(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation 10000/10000. Score = 0.9822999835014343\n",
      "Execution time: 0.0:0.0:29:111, processed 10000 frames, throughput: 343.51067327585497 fps.\n"
     ]
    }
   ],
   "source": [
    "metric = ... #TODO\n",
    "\n",
    "# You can evaluate your floating point model first \n",
    "evaluate(net, test_loader, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jeżeli wszystko działa poprawnie, a uzyskana dokładność jest na podobnym poziome jak podczas uczenia, możemy przejść do kwantyzacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Kwantyzacja modelu zmiennoprzecinkowego"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitis AI Quantizer dla Post Training Quantization składa się z dwóch części.\n",
    "Pierwszą częścią jest kalibracja (mode='calib') - Vitis AI Quantizer analizuje model i dostosowuje parametry kwantyzacji.\n",
    " \n",
    "Drugą częścią jest ewaluacja/testowanie (mode='test') - sprawdzana jest dokładność modelu (nie powinna być duża zmiana) i model jest eksportowany do formatu .xmodel.\n",
    "\n",
    "### Do obu części wykorzystamy funkcję quantize.\n",
    "\n",
    "Funkcja wykorzystuje kwantyzator dla PyTorch z gita Vitis AI: https://github.com/Xilinx/Vitis-AI/tree/1.4/tools/Vitis-AI-Quantizer/vai_q_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QFuGuk2NB4fz",
    "outputId": "3aea7dc1-a5e2-4f08-e7e2-f2af247666e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device,\n",
    "             dataloader,\n",
    "             evaluator):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader - for 'calib' must be batch_size == 1\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Quantization\", len(dataloader))\n",
    "    with tm:\n",
    "        # available in docker or after packaging \n",
    "        # vitis-AI-tools/..../pytorch../pytorch_nndct\n",
    "        # and installing the package\n",
    "        from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "        # model to device\n",
    "        model = float_model.to(device)\n",
    "\n",
    "        # Force to merge BN with CONV for better quantization accuracy\n",
    "        optimize = 1\n",
    "\n",
    "        rand_in = torch.randn(input_shape)\n",
    "        print(\"get qunatizer start\")\n",
    "        try:\n",
    "            quantizer = torch_quantizer(\n",
    "                quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "        except Exception as e:\n",
    "            print(\"exception:\")\n",
    "            print(e)\n",
    "            return\n",
    "        print(\"get qunatizer end\")\n",
    "\n",
    "        print(\"get quantized model start\")\n",
    "        quantized_model = quantizer.quant_model\n",
    "        print(\"get quantized model end\")\n",
    "\n",
    "        # evaluate\n",
    "        print(\"testing st\")\n",
    "        evaluate(quantized_model, dataloader, evaluator)\n",
    "        print(\"testing end\")\n",
    "\n",
    "        # export config\n",
    "        if quant_mode == 'calib':\n",
    "            print(\"export config\")\n",
    "            quantizer.export_quant_config()\n",
    "            print(\"export config end\")\n",
    "        # export model\n",
    "        if quant_mode == 'test':\n",
    "            print(\"export xmodel\")\n",
    "            quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "            print(\"export xmodel end\")\n",
    "    print(tm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczynamy od kalibracji. Jako wejście funkcji podajemy:\n",
    "- float_model - model zmiennoprzecinkowy, który uzyskaliśmy podczas uczenia,\n",
    "- input_shape - wymiar danych wejściowych w formacie [batch, CH, W, H],\n",
    "- quant_dir - folder do którego zostanie zapisany wynik kwantyzacji,\n",
    "- quant_mode - do wyboru 'calib' albo 'test',\n",
    "- device - urządzenie na którym zostaną wykonane obliczenia (CPU),\n",
    "- dataloader - dane na którym będą wykonane obliczenia,\n",
    "- evaluator - metryka, względem której będzie sprawdzana dokładność\n",
    "\n",
    "### Uwaga! Kwantyzacja w procesie kalibracji jest wolna. W przypadku dużych modeli i dużych wymiarów danych, nie można przesadzić z ilością danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UqCYtQlO3bll",
    "outputId": "7103e738-593a-4300-81ed-3acf6a7387e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing MiniResNet...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quant_dir/MiniResNet.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 10000/10000. Score = 0.9817000031471252\n",
      "Execution time: 5.0:0.0:30:416, processed 10000 frames, throughput: 30.264823388501018 fps.\n",
      "testing end\n",
      "export config\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quant_dir/quant_info.json)\u001b[0m\n",
      "export config end\n",
      "Execution time: 5.0:0.0:31:397, processed 10000 frames, throughput: 30.175226697334523 fps.\n"
     ]
    }
   ],
   "source": [
    "# Quantize model - calib - is slow\n",
    "\n",
    "#TODO\n",
    "quantize(float_model=..., \n",
    "         input_shape=...,\n",
    "         quant_dir='quant_dir',\n",
    "         quant_mode='calib',\n",
    "         device=...,\n",
    "         dataloader=...,\n",
    "         evaluator=...\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po udanej kalibracji, czas na testowanie i zapisanie modelu. Uruchamiamy funkcję ze zmienionym parametrem mode na 'test'.\n",
    "\n",
    "Proces ten jest szybszy od kalibracji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hW7mQGoL3blm",
    "outputId": "ffe3ef85-8078-4f53-87cc-62a3b036c36e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing MiniResNet...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quant_dir/MiniResNet.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 10000/10000. Score = 0.9824000000953674\n",
      "Execution time: 0.0:0.0:57:169, processed 10000 frames, throughput: 174.9184548731919 fps.\n",
      "testing end\n",
      "export xmodel\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Successfully convert 'MiniResNet' to xmodel.(quant_dir/MiniResNet_int.xmodel)\u001b[0m\n",
      "export xmodel end\n",
      "Execution time: 0.0:0.0:57:426, processed 10000 frames, throughput: 174.1343355669612 fps.\n"
     ]
    }
   ],
   "source": [
    "# Quantize model - test - is faster\n",
    "\n",
    "#TODO\n",
    "quantize(float_model=..., \n",
    "         input_shape=...,\n",
    "         quant_dir='quant_dir', # directory for quantizer results\n",
    "         quant_mode='test',\n",
    "         device=...,\n",
    "         dataloader=...,\n",
    "         evaluator=...\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po testowaniu, należy skompilować model. Podajemy odpowiednio parametry:\n",
    "\n",
    "- --xmodel - ścieżka do zapisanego modelu (zapisany jest w folderze podanym podczas kwantyzacji jako parametr 'quant_dir'\n",
    "- --arch - podajemy plik arch.json, który znajdował się w pliku. Jest to numer (fingerprint), który określa typ DPU sprzętu docelowego\n",
    "- --net_name - nazwa naszego modelu po kompilacji - dowolna\n",
    "- --output_dir - folder do którego zapisany zostanie model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5MApToNQ3bln",
    "outputId": "aa19a1c1-66a9-465f-cb22-43214b612ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: function\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2\n",
      "[UNILOG][INFO] Graph name: MiniResNet, with op num: 130\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/build/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/build/MiniResNet_qu.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 6f353093d80acd35dd41b979bfe4e8e1, and has been saved to \"/workspace/build/md5sum.txt\"\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "!vai_c_xir --xmodel 'quant_dir/MiniResNet_int.xmodel' --arch arch.json --net_name MiniResNet_qu --output_dir build"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ISD7IjSg3blp"
   },
   "source": [
    "Teraz przejdziemy do testowania modelu na sprzęcie docelowym.\n",
    "\n",
    "#### Wersja 1: Podłączenie do sieci\n",
    "Zanim podłączysz zasilanie do Kria, podepnij kabel USB do komputera, a kabel Ethernetowy do sieci, w której znajduje się komputer.\n",
    "\n",
    "Podłącz Kria do zasilania i poczekaj minutę, aż system się uruchomi.\n",
    "\n",
    "Uruchom `cutecom` z `sudo`. Włącz port odpowiadający do Kria. Jeżeli pojawi się napis `kria login:` zaloguj się:\n",
    "\n",
    "`login: ubuntu`\n",
    "\n",
    "`hasło: Xilinx123`\n",
    "\n",
    "Po zalogowaniu się, powinny pojawić się informacje systemowe. Nas interesuje adres `IPv4` dla `eth0`. Skopiuj go i dodaj do niego `:9090` - przykładowa wartość `192.168.1.26:9090`. Wklej to w przeglądarce. Powinien pojawić się kolejny Jupyter. Zaloguj się do niego hasłem:\n",
    "\n",
    "`xilinx`\n",
    "\n",
    "#### Wersja 2: Podłączenie do komputera\n",
    "Zanim podłączysz zasilanie do Kria, podepnij kabel USB do komputera oraz kabel Ethernetowy pomiędzy Kria a PC. Na PC włącz ustawienia sieci `Wired Setting` -> `IPv4` -> `Shared to other computers`. Włącz zasilanie płytki.\n",
    "Uruchom `cutecom` z `sudo`. Włącz port odpowiadający do Kria. Jeżeli pojawi się napis `kria login:` zaloguj się:\n",
    "\n",
    "`login: ubuntu`\n",
    "\n",
    "`hasło: Xilinx123`\n",
    "\n",
    "Po zalogowaniu się, powinny pojawić się informacje systemowe. Nas interesuje adres `inet` dla `eth0`. Skopiuj go i dodaj do niego `:9090` - przykładowa wartość `10.42.0.47:9090`. Wklej to w przeglądarce. Powinien pojawić się kolejny Jupyter. Zaloguj się do niego hasłem:\n",
    "\n",
    "`xilinx`\n",
    "\n",
    "#### Przesyłanie plików\n",
    "Stwórz nowy folder i nazwij go `PSRA_Lab`. Przenieś do niego odpowiednio pliki:\n",
    "- dpu.bit, \n",
    "- dpu.hwh, \n",
    "- dpu.xclbin, \n",
    "- eval_MNIST.npz lub tak jak nazwałeś swój plik z danymi do ewaluacji\n",
    "- MiniResNet_compiled.xmodel lub tak jak nazwałeś swój skompilowany plik\n",
    "\n",
    "Można to wykonać komendą `scp`, ale łatwiej jest przeciągnąć pliki z folderu do Jupyter Notebook'a."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UWAGA! \n",
    "\n",
    "Jeżeli wystąpi problem z adresami IPv4 Kria (po podłaczeniu kilku płytek do jednej sieci, każda z nich będzie miała taki sam adres), należy sprawdzić, czy działa komenda w konsoli `cutecom`:\n",
    "\n",
    "`ifconfig`\n",
    "\n",
    "Jeżeli nie, to należy zainstalować poprzez:\n",
    "\n",
    " `sudo apt install net-tools`.  \n",
    " \n",
    "Po tym należy odpiąć kabel Ethernet z Kria, poczekać kilka sekund i wpisać w konsole `cutecom`:\n",
    "\n",
    "`hostname -I`\n",
    "\n",
    "Jeżeli konsola nie zwróci żadnego błędu oraz żadnego aresu IP to wpisz w konsole `cutecom`:\n",
    "\n",
    "`sudo ifconfig eth0 192.168.1.x netmask 255.255.255.0`\n",
    "\n",
    "Tutaj podany adres powinien być taki sam jak przykładowa wartość wyżej. Ustawiamy wartość `x` na inną niż była np. 123. Chcemy uniknąć konfliktu pomiędzy płytkami ale również komputerami. Po tym znowu ponawiamy:\n",
    "\n",
    "`hostname -I`.\n",
    "\n",
    "Powinien pojawić sie ustawiony przez nas adres. Podpinamy kabel Ethernet i uruchamiamy w przeglądarce Jupyter z ustawionym adresem IP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Quantize.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "superpoint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
