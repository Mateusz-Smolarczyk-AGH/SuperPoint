{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZL5HI3Rj3blg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import local_utils\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from local_utils import ResidualBlock\n",
    "from torch import nn\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from types import SimpleNamespace\n",
    "import torch.quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W tej części ćwiczenia, wczytamy nauczony model zmiennoprzecinkowy MiniResNet, skwantyzujemy go do postaci stałoprzecinkowej i na koniec skompilujemy go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dane ewaluacyjne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczynamy od ponownego stworzenia generatora danych na bazie danych MNIST:\n",
    "\n",
    "Wystarczy nam sama część testowa. Ustawiamy `batch_size` na 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba obrazów w zbiorze testowym: 6\n"
     ]
    }
   ],
   "source": [
    "class SuperPointDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_paths = [str(f) for f in Path(image_folder).iterdir() if f.suffix == \".ppm\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.image_paths[idx]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "test_dataset = SuperPointDataset(\"./data\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"Liczba obrazów w zbiorze testowym: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo przygotujemy plik w formacie `.npz` na podstawie danych testowych. Wykorzystamy go do ewaluacji modelu na docelowej platformie Kria.\n",
    "\n",
    "Uzupełnij wektory `quantization_data` oraz `quantization_labels` danymi z `test_loadera`. Wykorzystaj do tego pętle `for` oraz `.append` (Przykład wykorzystania DataLoader'a z pętlą `for` przedstawiono w 1 części podczas wczytywania danych).\n",
    "\n",
    "Następnie każdy wektor z osobna połącz funkcją `torch.cat` z parametrem `dim=0` i przekonwertuj je do formatu `ndarray` za pomocą `.numpy()`. \n",
    "\n",
    "Zapisz je funkcją np.savez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantization_data = []\n",
    "#quantization_labels = []\n",
    "\n",
    "#TODO\n",
    "#Fill quantization vectors\n",
    "\n",
    "#train_X = ... #TODO\n",
    "#train_Y = ... #TODO\n",
    "\n",
    "#np.savez('eval_MNIST.npz', data=..., targets=...) #TODO\n",
    "\n",
    "#print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inicjalizacja modelu zmiennoprzecinkowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy taką samą klasę sieci Superpoint jak w pierwszej części ćwiczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_descriptors(keypoints, descriptors, s: int = 8):\n",
    "    \"\"\"Interpolate descriptors at keypoint locations\"\"\"\n",
    "    b, c, h, w = descriptors.shape\n",
    "    keypoints = (keypoints + 0.5) / (keypoints.new_tensor([w, h]) * s)\n",
    "    keypoints = keypoints * 2 - 1  # normalize to (-1, 1)\n",
    "    descriptors = torch.nn.functional.grid_sample(\n",
    "        descriptors, keypoints.view(b, 1, -1, 2), mode=\"bilinear\", align_corners=False\n",
    "    )\n",
    "    descriptors = torch.nn.functional.normalize(\n",
    "        descriptors.reshape(b, c, -1), p=2, dim=1\n",
    "    )\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def batched_nms(scores, nms_radius: int):\n",
    "    assert nms_radius >= 0\n",
    "\n",
    "    def max_pool(x):\n",
    "        return torch.nn.functional.max_pool2d(\n",
    "            x, kernel_size=nms_radius * 2 + 1, stride=1, padding=nms_radius\n",
    "        )\n",
    "\n",
    "    zeros = torch.zeros_like(scores)\n",
    "    max_mask = scores == max_pool(scores)\n",
    "    for _ in range(2):\n",
    "        supp_mask = max_pool(max_mask.float()) > 0\n",
    "        supp_scores = torch.where(supp_mask, zeros, scores)\n",
    "        new_max_mask = supp_scores == max_pool(supp_scores)\n",
    "        max_mask = max_mask | (new_max_mask & (~supp_mask))\n",
    "    return torch.where(max_mask, scores, zeros)\n",
    "\n",
    "\n",
    "def select_top_k_keypoints(keypoints, scores, k):\n",
    "    if k >= len(keypoints):\n",
    "        return keypoints, scores\n",
    "    scores, indices = torch.topk(scores, k, dim=0, sorted=True)\n",
    "    return keypoints[indices], scores\n",
    "\n",
    "\n",
    "class VGGBlock(nn.Sequential):\n",
    "    def __init__(self, c_in, c_out, kernel_size, relu=True):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        conv = nn.Conv2d(\n",
    "            c_in, c_out, kernel_size=kernel_size, stride=1, padding=padding\n",
    "        )\n",
    "        activation = nn.ReLU(inplace=True) if relu else nn.Identity()\n",
    "        bn = nn.BatchNorm2d(c_out, eps=0.001)\n",
    "        super().__init__(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"conv\", conv),\n",
    "                    (\"activation\", activation),\n",
    "                    (\"bn\", bn),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class SuperPoint(nn.Module):\n",
    "    default_conf = {\n",
    "        \"nms_radius\": 4,\n",
    "        \"max_num_keypoints\": 500,\n",
    "        \"detection_threshold\": 0.005,\n",
    "        \"remove_borders\": 4,\n",
    "        \"descriptor_dim\": 256,\n",
    "        \"channels\": [64, 64, 128, 128, 256],\n",
    "    }\n",
    "\n",
    "    def __init__(self, **conf):\n",
    "        super().__init__()\n",
    "        conf = {**self.default_conf, **conf}\n",
    "        self.conf = SimpleNamespace(**conf)\n",
    "        self.stride = 2 ** (len(self.conf.channels) - 2)\n",
    "        channels = [1, *self.conf.channels[:-1]]\n",
    "\n",
    "        backbone = []\n",
    "        for i, c in enumerate(channels[1:], 1):\n",
    "            layers = [VGGBlock(channels[i - 1], c, 3), VGGBlock(c, c, 3)]\n",
    "            if i < len(channels) - 1:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            backbone.append(nn.Sequential(*layers))\n",
    "        self.backbone = nn.Sequential(*backbone)\n",
    "\n",
    "        c = self.conf.channels[-1]\n",
    "        self.detector = nn.Sequential(\n",
    "            VGGBlock(channels[-1], c, 3),\n",
    "            VGGBlock(c, self.stride**2 + 1, 1, relu=False),\n",
    "        )\n",
    "        self.descriptor = nn.Sequential(\n",
    "            VGGBlock(channels[-1], c, 3),\n",
    "            VGGBlock(c, self.conf.descriptor_dim, 1, relu=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        image = data\n",
    "        if image.shape[1] == 3:  # RGB to gray\n",
    "            scale = image.new_tensor([0.299, 0.587, 0.114]).view(1, 3, 1, 1)\n",
    "            image = (image * scale).sum(1, keepdim=True)\n",
    "\n",
    "        features = self.backbone(image)\n",
    "        descriptors_dense = torch.nn.functional.normalize(\n",
    "            self.descriptor(features), p=2, dim=1\n",
    "        )\n",
    "\n",
    "        # Decode the detection scores\n",
    "        scores = self.detector(features)\n",
    "\n",
    "        scores = torch.nn.functional.softmax(scores, 1)[:, :-1]\n",
    "\n",
    "        b, _, h, w = scores.shape\n",
    "        scores = scores.permute(0, 2, 3, 1).reshape(b, h, w, self.stride, self.stride)\n",
    "        scores = scores.permute(0, 1, 3, 2, 4).reshape(\n",
    "            b, h * self.stride, w * self.stride\n",
    "        )\n",
    "\n",
    "        scores = batched_nms(scores, self.conf.nms_radius)\n",
    "       # lol = scores.numpy()\n",
    "       \n",
    "        # Discard keypoints near the image borders\n",
    "        if self.conf.remove_borders:\n",
    "            pad = self.conf.remove_borders\n",
    "            scores[:, :pad] = -1\n",
    "            scores[:, :, :pad] = -1\n",
    "            scores[:, -pad:] = -1\n",
    "            scores[:, :, -pad:] = -1\n",
    "\n",
    "        # Extract keypoints\n",
    "        if b > 1:\n",
    "            idxs = torch.where(scores > self.conf.detection_threshold)\n",
    "            mask = idxs[0] == torch.arange(b, device=scores.device)[:, None]\n",
    "        else:  # Faster shortcut\n",
    "            scores = scores.squeeze(0)\n",
    "            idxs = torch.where(scores > self.conf.detection_threshold)\n",
    "\n",
    "        # Convert (i, j) to (x, y)\n",
    "        keypoints_all = torch.stack(idxs[-2:], dim=-1).flip(1).float()\n",
    "        scores_all = scores[idxs]\n",
    "\n",
    "        keypoints = []\n",
    "        scores = []\n",
    "        descriptors = []\n",
    "        for i in range(b):\n",
    "            if b > 1:\n",
    "                k = keypoints_all[mask[i]]\n",
    "                s = scores_all[mask[i]]\n",
    "            else:\n",
    "                k = keypoints_all\n",
    "                s = scores_all\n",
    "            if self.conf.max_num_keypoints is not None:\n",
    "                k, s = select_top_k_keypoints(k, s, self.conf.max_num_keypoints)\n",
    "            d = sample_descriptors(k[None], descriptors_dense[i, None], self.stride)\n",
    "            keypoints.append(k)\n",
    "            scores.append(s)\n",
    "            descriptors.append(d.squeeze(0).transpose(0, 1))\n",
    "\n",
    "        return {\n",
    "            \"keypoints\": keypoints,\n",
    "            \"keypoint_scores\": scores,\n",
    "            \"descriptors\": descriptors,\n",
    "        }\n",
    "\n",
    "    \n",
    "class SuperPoint_short(nn.Module):\n",
    "    default_conf = {\n",
    "        \"nms_radius\": 4,\n",
    "        \"max_num_keypoints\": 500,\n",
    "        \"detection_threshold\": 0.005,\n",
    "        \"remove_borders\": 4,\n",
    "        \"descriptor_dim\": 256,\n",
    "        \"channels\": [64, 64, 128, 128, 256],\n",
    "    }\n",
    "\n",
    "    def __init__(self, **conf):\n",
    "        super().__init__()\n",
    "        conf = {**self.default_conf, **conf}\n",
    "        self.conf = conf\n",
    "        self.stride = 2 ** (len(self.conf[\"channels\"]) - 2)\n",
    "        channels = [1, *self.conf[\"channels\"][:-1]]\n",
    "\n",
    "        # Definicja QuantStub\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        backbone = []\n",
    "        for i, c in enumerate(channels[1:], 1):\n",
    "            layers = [VGGBlock(channels[i - 1], c, 3), VGGBlock(c, c, 3)]\n",
    "            if i < len(channels) - 1:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            backbone.append(nn.Sequential(*layers))\n",
    "        self.backbone = nn.Sequential(*backbone)\n",
    "\n",
    "        c = self.conf[\"channels\"][-1]\n",
    "        self.detector = nn.Sequential(\n",
    "            VGGBlock(channels[-1], c, 3),\n",
    "            VGGBlock(c, self.stride**2 + 1, 1, relu=False),\n",
    "        )\n",
    "        self.descriptor = nn.Sequential(\n",
    "            VGGBlock(channels[-1], c, 3),\n",
    "            VGGBlock(c, self.conf[\"descriptor_dim\"], 1, relu=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Kwantyzacja wejścia\n",
    "        image = self.quant(data)\n",
    "\n",
    "        features = self.backbone(image)\n",
    "        descriptors_dense = self.descriptor(features)\n",
    "\n",
    "        # Decode the detection scores\n",
    "        scores = self.detector(features)\n",
    "        # Dekwantyzacja przed wyjściem\n",
    "        return self.dequant(scores), self.dequant(descriptors_dense)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy model i wgrywamy wagi z pliku MNIST.pth. Zapisujemy go do urządzenia (w dockerze dostępny jest tylko CPU!) i ustawiamy go na ewaluację `.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "H1qAmZ7D3jMx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "detection_thresh = 0.005\n",
    "nms_radius = 5\n",
    "\n",
    "float_model = SuperPoint_short(detection_threshold=detection_thresh, nms_radius=nms_radius).eval()\n",
    "float_model.load_state_dict(torch.load(\"model_weights_legacy.pth\"))\n",
    "conf = float_model.conf\n",
    "input_shape = (1, 1, 64, 64)\n",
    "model = float_model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0+09b3f3d\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Inspector is initialized successfully with target:\u001b[0m\n",
      "name: DPUCZDX8G_ISA1_B4096\n",
      "type: DPUCZDX8G\n",
      "isa_version: 1\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Start to inspect model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing SuperPoint_short...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "██████████████████████████████████████████████████| 39/39 [00:00<00:00, 521.81it/s, OpInfo: name = return_0, type = Return]                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(inspect/SuperPoint_short.py)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: All the operators are assigned to the DPU(see more details in 'inspect/inspect_DPUCZDX8G_ISA1_B4096.txt')\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Dot image is generated.(inspect/inspect_DPUCZDX8G_ISA1_B4096.png)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Finish inspecting.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pytorch_nndct\n",
    "print(pytorch_nndct.__version__)\n",
    "from pytorch_nndct.apis import Inspector\n",
    "target = \"DPUCZDX8G_ISA1_B4096\"\n",
    "\n",
    "inspector = Inspector(target)\n",
    "rand_in = torch.randn(input_shape)\n",
    "inspector.inspect(model, (rand_in,), device=device, output_dir=\"inspect\", image_format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ewaluacja modelu zmiennoprzecinkowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przed przystąpieniem do kwantyzacji, wykonamy szybką ewaluację modelu zmiennoprzecinkowego. Sprawdzimy, czy dane są dobrze przygotowane i czy model został odpowiednio zapisany i wczytany. Wczytujemy metrykę Accuracy z `local_utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L4tpsFQX4jip"
   },
   "outputs": [],
   "source": [
    "def evaluate_orig(model,\n",
    "             dataloader):\n",
    "    tm = local_utils.TimeMeasurement(\"Evaluation\", len(dataloader))\n",
    "    with torch.no_grad(), tm:\n",
    "        score = 0.0\n",
    "        cntr = 0\n",
    "        for i, XY in enumerate(dataloader):\n",
    "            X = XY[0]\n",
    "            Y = XY[1:]\n",
    "            y_pred = model(X)\n",
    "            score = score*cntr + X.shape[0]*evaluator(y_pred, *Y)\n",
    "            cntr += X.shape[0]\n",
    "            score /= cntr\n",
    "            print(\"\\rEvaluation {}/{}. Score = {}\".format(i,len(dataloader), score),end='')\n",
    "        \n",
    "        print(\"\\rEvaluation {}/{}. Score = {}\".format(len(dataloader),len(dataloader), score),end='\\n')\n",
    "    print(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Porównuje działanie modelu przed i po kwantyzacji.\n",
    "\n",
    "    :param float_model: model przed kwantyzacją\n",
    "    :param quantized_model: model po kwantyzacji\n",
    "    :param dataloader: zbiór testowy\n",
    "    :param device: urządzenie (CPU/GPU)\n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Evaluation\", len(dataloader))\n",
    "    \n",
    "    model.to(device).eval()\n",
    "\n",
    "    diff_total, count = 0, 0\n",
    "    tensors = []\n",
    "    with torch.no_grad(), tm:\n",
    "        for i, (X, *Y) in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            # Forward pass dla modeli\n",
    "            scores, descriptors_dense = model(X)\n",
    "            scores = torch.nn.functional.softmax(scores, 1)[:, :-1]\n",
    "\n",
    "            b, _, h, w = scores.shape\n",
    "            scores = scores.permute(0, 2, 3, 1).reshape(b, h, w, 8, 8)\n",
    "            scores = scores.permute(0, 1, 3, 2, 4).reshape(\n",
    "                b, h * 8, w * 8\n",
    "            )\n",
    "            keypoints = post_processing_short(scores, descriptors_dense, model.conf)['keypoints']\n",
    "            data = torch.load(f'data/outputs_batch_{i}.pt')\n",
    "\n",
    "            data_keypoints = post_processing_short(data['scores'], data['descriptors_dense'], model.conf)['keypoints']\n",
    "\n",
    "            torch.save({'scores': scores, 'descriptors_dense': descriptors_dense}, f'outputs_batch_{i}.pt')\n",
    "\n",
    "            # Obliczenie różnicy\n",
    "            percent_diff = len(keypoints)/len(data_keypoints)\n",
    "            \n",
    "            # Aktualizacja wyników\n",
    "            batch_size = X.shape[0]\n",
    "            diff_total += percent_diff\n",
    "            count += batch_size\n",
    "\n",
    "            print(f\"\\rEvaluation {i+1}/{len(dataloader)} | keypoints_loss: {percent_diff:.6f}\", end='')\n",
    "\n",
    "    # Średnie wyniki\n",
    "    diff_avg = diff_total / count\n",
    "\n",
    "    print(f\"\\nFinal Evaluation: keypoints_loss = {diff_avg:.6f}\")\n",
    "    print(tm)\n",
    "\n",
    "    return diff_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "print(torch.__version__)\n",
    "\n",
    "detection_thresh = 0.005\n",
    "nms_radius = 5\n",
    "\n",
    "def match_descriptors(kp1, desc1, kp2, desc2):\n",
    "    # Match the keypoints with the warped_keypoints with nearest neighbor search\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "    matches = bf.match(desc1, desc2)\n",
    "    matches_idx = np.array([m.queryIdx for m in matches])\n",
    "    m_kp1 = [kp1[idx] for idx in matches_idx]\n",
    "    matches_idx = np.array([m.trainIdx for m in matches])\n",
    "    m_kp2 = [kp2[idx] for idx in matches_idx]\n",
    "\n",
    "    return m_kp1, m_kp2, matches\n",
    "\n",
    "def compute_homography(matched_kp1, matched_kp2):\n",
    "    matched_pts1 = cv2.KeyPoint_convert(matched_kp1)\n",
    "    matched_pts2 = cv2.KeyPoint_convert(matched_kp2)\n",
    "\n",
    "    # Estimate the homography between the matches using RANSAC\n",
    "    H, inliers = cv2.findHomography(matched_pts1,\n",
    "                                    matched_pts2,\n",
    "                                    cv2.RANSAC)\n",
    "    inliers = inliers.flatten()\n",
    "    return H, inliers\n",
    "\n",
    "def post_processing_short(scores, descriptors_dense, conf):\n",
    "    b = scores.shape[0]\n",
    "    scores = batched_nms(scores, conf['nms_radius'])\n",
    "\n",
    "    # Discard keypoints near the image borders\n",
    "    if conf['remove_borders']:\n",
    "        pad = conf['remove_borders']\n",
    "        scores[:, :pad] = -1\n",
    "        scores[:, :, :pad] = -1\n",
    "        scores[:, -pad:] = -1\n",
    "        scores[:, :, -pad:] = -1\n",
    "\n",
    "    scores = scores.squeeze(0)\n",
    "    idxs = torch.where(scores > conf['detection_threshold'])\n",
    "\n",
    "    # Convert (i, j) to (x, y)\n",
    "    keypoints_all = torch.stack(idxs[-2:], dim=-1).flip(1).float()\n",
    "    scores_all = scores[idxs]\n",
    "\n",
    "    keypoints = []\n",
    "    scores = []\n",
    "    descriptors = []\n",
    "    for i in range(b):\n",
    "        if b > 1:\n",
    "            k = keypoints_all[mask[i]]\n",
    "            s = scores_all[mask[i]]\n",
    "        else:\n",
    "            k = keypoints_all\n",
    "            s = scores_all\n",
    "        if conf['max_num_keypoints'] is not None:\n",
    "            k, s = select_top_k_keypoints(k, s, conf['max_num_keypoints'])\n",
    "        d = sample_descriptors(k[None], descriptors_dense[i, None], 2 ** (len(conf['channels']) - 2))\n",
    "        keypoints.append(k)\n",
    "        scores.append(s)\n",
    "        descriptors.append(d.squeeze(0).transpose(0, 1))\n",
    "\n",
    "    return {\n",
    "        \"keypoints\": keypoints,\n",
    "        \"keypoint_scores\": scores,\n",
    "        \"descriptors\": descriptors,\n",
    "    }\n",
    "\n",
    "\n",
    "def show_comparison(tensors1, tensors2, img1_orig, img2_orig, model):\n",
    "\n",
    "    # Run inference for both images\n",
    "    tensors = [tensors1, tensors2]\n",
    "    keypoints_list = []\n",
    "    desc_list = []\n",
    "\n",
    "    for tensor in tensors:\n",
    "        pred_th_1 = post_processing_short(tensor['scores'], tensor['descriptors_dense'], model.conf)\n",
    "        descriptors = pred_th_1['descriptors'][0]\n",
    "        points_th = pred_th_1['keypoints'][0]\n",
    "        keypoints_np = np.array(points_th)  # Konwersja do NumPy\n",
    "        keypoints = [cv2.KeyPoint(float(p[0]), float(p[1]), 1) for p in keypoints_np]\n",
    "        keypoints_list.append(keypoints)\n",
    "        desc_list.append(descriptors.cpu().detach().numpy().astype(np.float32)\n",
    ")\n",
    "\n",
    "    m_kp1, m_kp2, matches = match_descriptors(keypoints_list[0], desc_list[0], keypoints_list[1], desc_list[1])\n",
    "    H, inliers = compute_homography(m_kp1, m_kp2)\n",
    "\n",
    "    # Draw SuperPoint matches\n",
    "    matches = np.array(matches)[inliers.astype(bool)].tolist()\n",
    "    matched_img = cv2.drawMatches(img1_orig, keypoints_list[0], img2_orig, keypoints_list[1], matches,\n",
    "                                    None, matchColor=(0, 255, 0),\n",
    "                                    singlePointColor=(0, 0, 255))\n",
    "\n",
    "    return matched_img\n",
    "\n",
    "def visualize(model):\n",
    "    img1 = cv2.imread(f'data/1.ppm', cv2.IMREAD_COLOR)\n",
    "    img1 = cv2.resize(img1, (300, 200))\n",
    "    img2 = cv2.imread(f'data/2.ppm', cv2.IMREAD_COLOR)\n",
    "    img2 = cv2.resize(img2, (300, 200))\n",
    "    data0 = torch.load(f'data/outputs_batch_0.pt')\n",
    "    data1 = torch.load(f'data/outputs_batch_1.pt')\n",
    "    matched = show_comparison(data0, data1, img1, img2, model)\n",
    "    cv2.imwrite(\"data/matched_image_quant.png\", matched)\n",
    "    print(\"Image saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.7/site-packages/torch/_tensor.py:493: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "requested resize to torch.Size([1, 64, 64]) (torch.Size([1, 64, 64]) elements in total), but the given tensor has a size of 1x200x296 (59200 elements). autograd's resize can only change the shape of a given tensor, while preserving the number of elements. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_173/2086326800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# You can evaluate your floating point model first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_173/152272050.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Obliczenie różnicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, *sizes)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"non-inplace resize is deprecated\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresize_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.7/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, tensor, sizes)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                 \u001b[0;34m\"tensor, while preserving the number of elements. \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;34m'x'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 'x'.join(map(str, tensor.size())), tensor.numel()))\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_quantized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: requested resize to torch.Size([1, 64, 64]) (torch.Size([1, 64, 64]) elements in total), but the given tensor has a size of 1x200x296 (59200 elements). autograd's resize can only change the shape of a given tensor, while preserving the number of elements. "
     ]
    }
   ],
   "source": [
    "# You can evaluate your floating point model first \n",
    "evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jeżeli wszystko działa poprawnie, a uzyskana dokładność jest na podobnym poziome jak podczas uczenia, możemy przejść do kwantyzacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Kwantyzacja modelu zmiennoprzecinkowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitis AI Quantizer dla Post Training Quantization składa się z dwóch części.\n",
    "Pierwszą częścią jest kalibracja (mode='calib') - Vitis AI Quantizer analizuje model i dostosowuje parametry kwantyzacji.\n",
    " \n",
    "Drugą częścią jest ewaluacja/testowanie (mode='test') - sprawdzana jest dokładność modelu (nie powinna być duża zmiana) i model jest eksportowany do formatu .xmodel.\n",
    "\n",
    "### Do obu części wykorzystamy funkcję quantize.\n",
    "\n",
    "Funkcja wykorzystuje kwantyzator dla PyTorch z gita Vitis AI: https://github.com/Xilinx/Vitis-AI/tree/1.4/tools/Vitis-AI-Quantizer/vai_q_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QFuGuk2NB4fz",
    "outputId": "3aea7dc1-a5e2-4f08-e7e2-f2af247666e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device,\n",
    "             dataloader):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader - for 'calib' must be batch_size == 1\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Quantization\", len(dataloader))\n",
    "    from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "\n",
    "    with tm:\n",
    "        # model to device\n",
    "        model = float_model.to(device)\n",
    "\n",
    "        # Force to merge BN with CONV for better quantization accuracy\n",
    "        optimize = 1\n",
    "\n",
    "        rand_in = torch.randn(input_shape)\n",
    "        print(\"get qunatizer start\")\n",
    "        try:\n",
    "            quantizer = torch_quantizer(\n",
    "                quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "        except Exception as e:\n",
    "            print(\"exception:\")\n",
    "            print(e)\n",
    "            return\n",
    "        print(\"get qunatizer end\")\n",
    "\n",
    "        print(\"get quantized model start\")\n",
    "        quantized_model = quantizer.quant_model\n",
    "        print(\"get quantized model end\")\n",
    "\n",
    "        # evaluate\n",
    "        print(\"testing start\")\n",
    "        diff = evaluate(quantized_model, dataloader, device)\n",
    "        print(f\"Testing finished: diff = {diff:.6f}\")\n",
    "        print(\"testing end\")\n",
    "\n",
    "        # export config\n",
    "        if quant_mode == 'calib':\n",
    "            print(\"export config\")\n",
    "            quantizer.export_quant_config()\n",
    "            print(\"export config end\")\n",
    "        # export model\n",
    "        if quant_mode == 'test':\n",
    "            print(\"export xmodel\")\n",
    "            visualize(float_model)\n",
    "            quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "            print(\"export xmodel end\")\n",
    "    print(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczynamy od kalibracji. Jako wejście funkcji podajemy:\n",
    "- float_model - model zmiennoprzecinkowy, który uzyskaliśmy podczas uczenia,\n",
    "- input_shape - wymiar danych wejściowych w formacie [batch, CH, W, H],\n",
    "- quant_dir - folder do którego zostanie zapisany wynik kwantyzacji,\n",
    "- quant_mode - do wyboru 'calib' albo 'test',\n",
    "- device - urządzenie na którym zostaną wykonane obliczenia (CPU),\n",
    "- dataloader - dane na którym będą wykonane obliczenia,\n",
    "- evaluator - metryka, względem której będzie sprawdzana dokładność\n",
    "\n",
    "### Uwaga! Kwantyzacja w procesie kalibracji jest wolna. W przypadku dużych modeli i dużych wymiarów danych, nie można przesadzić z ilością danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po udanej kalibracji, czas na testowanie i zapisanie modelu. Uruchamiamy funkcję ze zmienionym parametrem mode na 'test'.\n",
    "\n",
    "Proces ten jest szybszy od kalibracji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "hW7mQGoL3blm",
    "outputId": "ffe3ef85-8078-4f53-87cc-62a3b036c36e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing SuperPoint_short...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "██████████████████████████████████████████████████| 39/39 [00:00<00:00, 550.80it/s, OpInfo: name = return_0, type = Return]                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quant_dir/SuperPoint_short.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (296) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_173/3800746419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m          \u001b[0mquant_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'calib'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m          \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m          dataloader=test_loader)\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_173/1894923174.py\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(float_model, input_shape, quant_dir, quant_mode, device, dataloader)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Testing finished: MAE = {mae:.6f}, MSE = {mse:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_173/2952153127.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Obliczenie różnicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (296) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Quantize model - test - is faster\n",
    "\n",
    "#TODO\n",
    "quantize(float_model=model, \n",
    "         input_shape=input_shape,\n",
    "         quant_dir='quant_dir', # directory for quantizer results\n",
    "         quant_mode='calib',\n",
    "         device=device,\n",
    "         dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved\n"
     ]
    }
   ],
   "source": [
    "visualize(float_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po testowaniu, należy skompilować model. Podajemy odpowiednio parametry:\n",
    "\n",
    "- --xmodel - ścieżka do zapisanego modelu (zapisany jest w folderze podanym podczas kwantyzacji jako parametr 'quant_dir'\n",
    "- --arch - podajemy plik arch.json, który znajdował się w pliku. Jest to numer (fingerprint), który określa typ DPU sprzętu docelowego\n",
    "- --net_name - nazwa naszego modelu po kompilacji - dowolna\n",
    "- --output_dir - folder do którego zapisany zostanie model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "5MApToNQ3bln",
    "outputId": "aa19a1c1-66a9-465f-cb22-43214b612ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: function\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B4096\n",
      "[UNILOG][INFO] Graph name: SuperPoint_short, with op num: 150\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "\u001b[0;31m[UNILOG][FATAL][XCOM_DATA_OUTRANGE][Data value is out of range!] \n",
      "\u001b[m*** Check failure stack trace: ***\n",
      "Aborted (core dumped)\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "!vai_c_xir --xmodel 'quant_dir/SuperPoint_short_int.xmodel' --arch arch.json --net_name SuperPoint_qu --output_dir build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISD7IjSg3blp"
   },
   "source": [
    "Teraz przejdziemy do testowania modelu na sprzęcie docelowym.\n",
    "\n",
    "#### Wersja 1: Podłączenie do sieci\n",
    "Zanim podłączysz zasilanie do Kria, podepnij kabel USB do komputera, a kabel Ethernetowy do sieci, w której znajduje się komputer.\n",
    "\n",
    "Podłącz Kria do zasilania i poczekaj minutę, aż system się uruchomi.\n",
    "\n",
    "Uruchom `cutecom` z `sudo`. Włącz port odpowiadający do Kria. Jeżeli pojawi się napis `kria login:` zaloguj się:\n",
    "\n",
    "`login: ubuntu`\n",
    "\n",
    "`hasło: Xilinx123`\n",
    "\n",
    "Po zalogowaniu się, powinny pojawić się informacje systemowe. Nas interesuje adres `IPv4` dla `eth0`. Skopiuj go i dodaj do niego `:9090` - przykładowa wartość `192.168.1.26:9090`. Wklej to w przeglądarce. Powinien pojawić się kolejny Jupyter. Zaloguj się do niego hasłem:\n",
    "\n",
    "`xilinx`\n",
    "\n",
    "#### Wersja 2: Podłączenie do komputera\n",
    "Zanim podłączysz zasilanie do Kria, podepnij kabel USB do komputera oraz kabel Ethernetowy pomiędzy Kria a PC. Na PC włącz ustawienia sieci `Wired Setting` -> `IPv4` -> `Shared to other computers`. Włącz zasilanie płytki.\n",
    "Uruchom `cutecom` z `sudo`. Włącz port odpowiadający do Kria. Jeżeli pojawi się napis `kria login:` zaloguj się:\n",
    "\n",
    "`login: ubuntu`\n",
    "\n",
    "`hasło: Xilinx123`\n",
    "\n",
    "Po zalogowaniu się, powinny pojawić się informacje systemowe. Nas interesuje adres `inet` dla `eth0`. Skopiuj go i dodaj do niego `:9090` - przykładowa wartość `10.42.0.47:9090`. Wklej to w przeglądarce. Powinien pojawić się kolejny Jupyter. Zaloguj się do niego hasłem:\n",
    "\n",
    "`xilinx`\n",
    "\n",
    "#### Przesyłanie plików\n",
    "Stwórz nowy folder i nazwij go `PSRA_Lab`. Przenieś do niego odpowiednio pliki:\n",
    "- dpu.bit, \n",
    "- dpu.hwh, \n",
    "- dpu.xclbin, \n",
    "- eval_MNIST.npz lub tak jak nazwałeś swój plik z danymi do ewaluacji\n",
    "- MiniResNet_compiled.xmodel lub tak jak nazwałeś swój skompilowany plik\n",
    "\n",
    "Można to wykonać komendą `scp`, ale łatwiej jest przeciągnąć pliki z folderu do Jupyter Notebook'a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UWAGA! \n",
    "\n",
    "Jeżeli wystąpi problem z adresami IPv4 Kria (po podłaczeniu kilku płytek do jednej sieci, każda z nich będzie miała taki sam adres), należy sprawdzić, czy działa komenda w konsoli `cutecom`:\n",
    "\n",
    "`ifconfig`\n",
    "\n",
    "Jeżeli nie, to należy zainstalować poprzez:\n",
    "\n",
    " `sudo apt install net-tools`.  \n",
    " \n",
    "Po tym należy odpiąć kabel Ethernet z Kria, poczekać kilka sekund i wpisać w konsole `cutecom`:\n",
    "\n",
    "`hostname -I`\n",
    "\n",
    "Jeżeli konsola nie zwróci żadnego błędu oraz żadnego aresu IP to wpisz w konsole `cutecom`:\n",
    "\n",
    "`sudo ifconfig eth0 192.168.1.x netmask 255.255.255.0`\n",
    "\n",
    "Tutaj podany adres powinien być taki sam jak przykładowa wartość wyżej. Ustawiamy wartość `x` na inną niż była np. 123. Chcemy uniknąć konfliktu pomiędzy płytkami ale również komputerami. Po tym znowu ponawiamy:\n",
    "\n",
    "`hostname -I`.\n",
    "\n",
    "Powinien pojawić sie ustawiony przez nas adres. Podpinamy kabel Ethernet i uruchamiamy w przeglądarce Jupyter z ustawionym adresem IP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Quantize.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
